{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "\n",
    "from keras.layers import Embedding, Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "random.seed(1228)\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 1 [2 балла] Подготовка данных\n",
    "1. Прочитайте размеченные данные Открытого корпуса, используя nltk.corpus.reader.conll.ConllCorpusReader\n",
    "2. Посчитайте количество предложений и число тегов частей речи;\n",
    "3. Сформируйте тестовое и обучающее множество: первые 3/4 данных – обучающее множество;\n",
    "\n",
    "Для каждого слова:\n",
    "1. Определите его окно (слова слева и справа) размера $k$;\n",
    "2. Сформируйте его вектор признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.conll import ConllCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('«', 'PUNCT', '_'), ('Школа', 'NOUN', '_'), ('злословия', 'NOUN', '_'), ('»', 'PUNCT', '_'), ('учит', 'VERB', '_'), ('прикусить', 'VERB', '_'), ('язык', 'NOUN', '_')], [('Сохранится', 'VERB', '_'), ('ли', 'PART', '_'), ('градус', 'NOUN', '_'), ('дискуссии', 'NOUN', '_'), ('в', 'ADP', '_'), ('новом', 'ADJ', '_'), ('сезоне', 'NOUN', '_'), ('?', 'PUNCT', '_')], ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['ignore', 'words', 'ignore', 'pos', 'chunk']\n",
    "train_reader = ConllCorpusReader(root = '.', fileids = 'unamb_sent_14_6.conllu', columntypes = columns)\n",
    "\n",
    "\n",
    "sents = train_reader.iob_sents()\n",
    "train_reader.iob_sents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Считаем количество предложений и разных тэгов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38508"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sents) # Предложений в корпусе 38508"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PUNCT',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'PUNCT',\n",
       " 'VERB',\n",
       " 'VERB',\n",
       " 'NOUN',\n",
       " 'VERB',\n",
       " 'PART',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'ADP',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'PUNCT']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#вытащим все тэги\n",
    "pos = [w[1] for sent in sents for w in sent]\n",
    "pos[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# вот столько у нас разных тэгов\n",
    "len(set(pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# а теперь посмотрим на частотное распределение тэгов \n",
    "from nltk import FreqDist\n",
    "fd_pos = FreqDist(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'ADJ': 47487,\n",
       "          'ADP': 42835,\n",
       "          'ADV': 13079,\n",
       "          'CONJ': 21942,\n",
       "          'DET': 12689,\n",
       "          'INTJ': 452,\n",
       "          'NOUN': 121793,\n",
       "          'NUM': 10173,\n",
       "          'PART': 8923,\n",
       "          'PRON': 9067,\n",
       "          'PROPN': 14889,\n",
       "          'PUNCT': 91323,\n",
       "          'VERB': 41538,\n",
       "          'X': 21393})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы видим, самые частые -- прилагательные и предлоги (о_О)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сформируем тестовую и тренировочную выборки для тэгов\n",
    "\n",
    "Итак, вектор POS у нас есть: `pos`. Теперь сделаем вектор признаков. Для удобства мы решили убрать информацию о границах предложений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['«',\n",
       " 'школа',\n",
       " 'злословия',\n",
       " '»',\n",
       " 'учит',\n",
       " 'прикусить',\n",
       " 'язык',\n",
       " 'сохранится',\n",
       " 'ли',\n",
       " 'градус',\n",
       " 'дискуссии',\n",
       " 'в',\n",
       " 'новом',\n",
       " 'сезоне',\n",
       " '?',\n",
       " 'великолепная',\n",
       " '«',\n",
       " 'школа',\n",
       " 'злословия',\n",
       " '»']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [word[0].lower() for sent in sents for word in sent]\n",
    "words[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Извлечение признаков:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddingsPath = './wiki.ru.vec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(path):\n",
    "    embeddings = {}\n",
    "    with open(embeddingsPath) as f:\n",
    "        for line in f:\n",
    "            word, vec = line.split(' ', 1)\n",
    "            embeddings[word] = [float(num) for num in vec.split()]\n",
    "    return embeddings\n",
    "\n",
    "embeddings = extract_embeddings(embeddingsPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.32315,\n",
       " 0.91456,\n",
       " 0.13797,\n",
       " 0.61075,\n",
       " -0.28406,\n",
       " -0.47918,\n",
       " -0.27341,\n",
       " 0.17947,\n",
       " 0.54726,\n",
       " -0.47914,\n",
       " -0.20418,\n",
       " 0.12833,\n",
       " 0.1399,\n",
       " 0.26005,\n",
       " 0.53394]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings['школа'][:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings['школа'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a strange fix\n",
    "embeddings['«'] = embeddings['\"']\n",
    "embeddings['»'] = embeddings['\"']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаём вектора для начала и конца (и плэйсхолдер на случай если в эмбеддингах слова нет)\n",
    "MOF = [0] * 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k == 2\n",
    "def featues_k2(tokens):\n",
    "    features = []\n",
    "    for i, word in enumerate(tokens):\n",
    "        if word in embeddings:\n",
    "            cur_word = embeddings[word]\n",
    "        else:\n",
    "            cur_word = MOF\n",
    "        if i > 0 and tokens[i - 1] in embeddings:\n",
    "            prev_word = embeddings[tokens[i - 1]]\n",
    "        else:\n",
    "            prev_word = MOF\n",
    "        if i < len(tokens) - 1 and tokens[i + 1] in embeddings:\n",
    "            next_word = embeddings[tokens[i + 1]]\n",
    "        else:\n",
    "            next_word = MOF\n",
    "        features.append(cur_word + prev_word + next_word)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = featues_k2(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features) == len(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "343185\n",
      "114398\n"
     ]
    }
   ],
   "source": [
    "boundary = (len(pos) // 4) * 3\n",
    "y_train = pos[:boundary]\n",
    "X_train = features[:boundary]\n",
    "y_test = pos[boundary:]\n",
    "X_test = features[boundary:]\n",
    "\n",
    "print(len(y_train))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 2 [4 баллов] Архитектура нейронной сети\n",
    "\n",
    "Архитектура нейронной сети состоит из следующих слов:\n",
    "1. Входной слой: нейронная сеть получает на вход вектор признаков, состоящий из $k$ конкатенированных эмбеддингов;/\n",
    "2. Скрытый слой: $n_h$ нейронов и нелинейная функция активации $\\theta$;\n",
    "3. Выходной слой:  $|T|$ нейронов для итоговой классификации.\n",
    "\n",
    "Обучите нейронную сеть на обучающих данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "я бы поставила семь, конечно, но можно и три пока а это для последнего задания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_LENGTH = 1800\n",
    "VOCABULARY_SIZE = 250000\n",
    "EMBEDDING_DIM = 300\n",
    "DIMS = 250\n",
    "MAX_FEATURES = 5000\n",
    "batch_size = 32\n",
    "\n",
    "nb_filter = 250\n",
    "filter_length = 3\n",
    "hidden_dims = 250\n",
    "nb_epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_s' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-b0f3b10b8654>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequences_to_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'count'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_corpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequences_to_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'count'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_s' is not defined"
     ]
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(train_s.body)\n",
    "X_train = tokenizer.sequences_to_matrix(sequences, mode='count')\n",
    "sequences = tokenizer.texts_to_sequences(test_corpus.body)\n",
    "X_test = tokenizer.sequences_to_matrix(sequences, mode='count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотреть какая именно архитектура сенны"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(VOCABULARY_SIZE, EMBEDDING_DIM, input_length=TEXT_LENGTH))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(5, activation = 'softmax'))\n",
    "model.add(Dropout(0.5))\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=nb_epoch, batch_size=batch_size,  validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
